{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**INTRODUCTION**\n\nHi all, this kernel is an intorduction to text classification using deep leanring. It took some time for the deep learning approaches to make a mark on textual data but since then the impact of deep learning on NLP has had a vertical graph. \n\nIn this kernel we will get our hands dirty with a well in demand problem of text/document classification, around 2014 yoon kim et al. started to experiment with the relevance of CNN in the field of NLP and since then there has been no looking back. In the paper \"[Convolutional Neural Networks for Sentence Classification](http://arxiv.org/pdf/1408.5882.pdf)\" yoon kim et al. experiments with multiple CNN models (single channel, multiple channel) on top of word embeddings for text classification.\n\nFor the sake of simplicity we will start off with a single channel model with pretrasined Glove embeddings. The data set used is the famous [20_newsgroup dataset](http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/news20.html)(original dataset link).\n\nIn this kernel we will first learn about the processing of dataset, followed by a keras implementation of text classification using the preexisting Glove embeddings. \n","metadata":{"_uuid":"33f5679b5e95039a32bd91c4d9fe6e55c3e17462"}},{"cell_type":"markdown","source":"**THE APPROACH**\n\nThe idea presented follows a flow like : \n<a href=\"https://imgur.com/xLrP6IM\"><img src=\"https://i.imgur.com/xLrP6IM.png\" title=\"source: imgur.com\" style=\"width:400px;height:600px;\"/></a>\n\n\nWe basically add different convolution layers of filter sizes [3, 4, 5], this somewhat emulates different skip-gram models where different filter sizes essentially means the number of words the filter is being applied to. ","metadata":{"_uuid":"a80121ade4ecf6c4caa264a5fd5cd5981778119a"}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\nfrom keras.layers import MaxPool1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam","metadata":{"_uuid":"0750388a2837b4425e5f1dd11da60a8f9b30c7b4","trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"# just to make sure the dataset is added properly \n!ls '../input/20-newsgroup-original/20_newsgroup/20_newsgroup/'\n","metadata":{"_uuid":"daf453aefbb50f26e525042203cfcbf4b3976c57","trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"alt.atheism\t\t  rec.autos\t      sci.space\ncomp.graphics\t\t  rec.motorcycles     soc.religion.christian\ncomp.os.ms-windows.misc   rec.sport.baseball  talk.politics.guns\ncomp.sys.ibm.pc.hardware  rec.sport.hockey    talk.politics.mideast\ncomp.sys.mac.hardware\t  sci.crypt\t      talk.politics.misc\ncomp.windows.x\t\t  sci.electronics     talk.religion.misc\nmisc.forsale\t\t  sci.med\n","output_type":"stream"}]},{"cell_type":"code","source":"# the dataset path\nTEXT_DATA_DIR = r'../input/20-newsgroup-original/20_newsgroup/20_newsgroup/'\n#the path for Glove embeddings\nGLOVE_DIR = r'../input/glove6b/'\n# make the max word length to be constant\nMAX_WORDS = 10000\nMAX_SEQUENCE_LENGTH = 1000\n# the percentage of train test split to be applied\nVALIDATION_SPLIT = 0.20\n# the dimension of vectors to be used\nEMBEDDING_DIM = 100\n# filter sizes of the different conv layers \nfilter_sizes = [3,4,5]\nnum_filters = 512\nembedding_dim = 100\n# dropout probability\ndrop = 0.5\nbatch_size = 30\nepochs = 2","metadata":{"_uuid":"35297be0266ec89dc1786312025a26458be584d6","trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**DATASET STRUCTURE**\n\nThe dataset is present in a hierarchical structure, i.e. all files of a given class are located in their respective folders and each datapoint has its own '.txt' file.\n\n* First we go through the entire dataset to build our text list and label list. \n* Followed by this we tokenize the entire data using Tokenizer, which is a part of keras.preprocessing.text.\n* We then add padding to the sequences to make them of a uniform length.","metadata":{"_uuid":"fa68305c64bc844cd86e776b3cb37e3661f6f652"}},{"cell_type":"code","source":"## preparing dataset\n\n\ntexts = []  # list of text samples\nlabels_index = {}  # dictionary mapping label name to numeric id\nlabels = []  # list of label ids\nfor name in sorted(os.listdir(TEXT_DATA_DIR)):\n    path = os.path.join(TEXT_DATA_DIR, name)\n    if os.path.isdir(path):\n        label_id = len(labels_index)\n        labels_index[name] = label_id\n        for fname in sorted(os.listdir(path)):\n            if fname.isdigit():\n                fpath = os.path.join(path, fname)\n                if sys.version_info < (3,):\n                    f = open(fpath)\n                else:\n                    f = open(fpath, encoding='latin-1')\n                t = f.read()\n                i = t.find('\\n\\n')  # skip header\n                if 0 < i:\n                    t = t[i:]\n                texts.append(t)\n                f.close()\n                labels.append(label_id)\nprint(labels_index)\n\nprint('Found %s texts.' % len(texts))","metadata":{"_uuid":"8f9972b8b95a38df4e08227b5a638bd675d7c945","trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"{'alt.atheism': 0, 'comp.graphics': 1, 'comp.os.ms-windows.misc': 2, 'comp.sys.ibm.pc.hardware': 3, 'comp.sys.mac.hardware': 4, 'comp.windows.x': 5, 'misc.forsale': 6, 'rec.autos': 7, 'rec.motorcycles': 8, 'rec.sport.baseball': 9, 'rec.sport.hockey': 10, 'sci.crypt': 11, 'sci.electronics': 12, 'sci.med': 13, 'sci.space': 14, 'soc.religion.christian': 15, 'talk.politics.guns': 16, 'talk.politics.mideast': 17, 'talk.politics.misc': 18, 'talk.religion.misc': 19}\nFound 19997 texts.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(texts[1000])","metadata":{"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"\n\n\n\t\t\tCALL FOR PRESENTATIONS\n\t\n      NAVY SCIENTIFIC VISUALIZATION AND VIRTUAL REALITY SEMINAR\n\n\t\t\tTuesday, June 22, 1993\n\n\t    Carderock Division, Naval Surface Warfare Center\n\t      (formerly the David Taylor Research Center)\n\t\t\t  Bethesda, Maryland\n\nSPONSOR: NESS (Navy Engineering Software System) is sponsoring a \none-day Navy Scientific Visualization and Virtual Reality Seminar.  \nThe purpose of the seminar is to present and exchange information for\nNavy-related scientific visualization and virtual reality programs, \nresearch, developments, and applications.\n\nPRESENTATIONS: Presentations are solicited on all aspects of \nNavy-related scientific visualization and virtual reality.  All \ncurrent work, works-in-progress, and proposed work by Navy \norganizations will be considered.  Four types of presentations are \navailable.\n\n     1. Regular presentation: 20-30 minutes in length\n     2. Short presentation: 10 minutes in length\n     3. Video presentation: a stand-alone videotape (author need not \n\tattend the seminar)\n     4. Scientific visualization or virtual reality demonstration (BYOH)\n\nAccepted presentations will not be published in any proceedings, \nhowever, viewgraphs and other materials will be reproduced for \nseminar attendees.\n\nABSTRACTS: Authors should submit a one page abstract and/or videotape to:\n\n     Robert Lipman\n     Naval Surface Warfare Center, Carderock Division\n     Code 2042\n     Bethesda, Maryland  20084-5000\n\n     VOICE (301) 227-3618;  FAX (301) 227-5753  \n     E-MAIL  lipman@oasys.dt.navy.mil\n\nAuthors should include the type of presentation, their affiliations, \naddresses, telephone and FAX numbers, and addresses.  Multi-author \npapers should designate one point of contact.\n\nDEADLINES: The abstact submission deadline is April 30, 1993.  \nNotification of acceptance will be sent by May 14, 1993.  \nMaterials for reproduction must be received by June 1, 1993.\n\nFor further information, contact Robert Lipman at the above address.\n\n\t  PLEASE DISTRIBUTE AS WIDELY AS POSSIBLE, THANKS.\n\n\n\n\nRobert Lipman                     | Internet: lipman@oasys.dt.navy.mil\nDavid Taylor Model Basin - CDNSWC |       or: lip@ocean.dt.navy.mil\nComputational Signatures and      | Voicenet: (301) 227-3618\n   Structures Group, Code 2042    | Factsnet: (301) 227-5753\nBethesda, Maryland  20084-5000    | Phishnet: stockings@long.legs\n\t\t\t\t   \nThe sixth sick shiek's sixth sheep's sick.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer  = Tokenizer(num_words = MAX_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences =  tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nprint(\"unique words : {}\".format(len(word_index)))\n\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\nprint(labels)","metadata":{"_uuid":"f3058f0c6703374a384d8720712cb2151e44e8ca","trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"unique words : 174074\nShape of data tensor: (19997, 1000)\nShape of label tensor: (19997, 20)\n[[1. 0. 0. ... 0. 0. 0.]\n [1. 0. 0. ... 0. 0. 0.]\n [1. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 1.]\n [0. 0. 0. ... 0. 0. 1.]\n [0. 0. 0. ... 0. 0. 1.]]\n","output_type":"stream"}]},{"cell_type":"code","source":"x_test=data[1000]\ny_test=labels[1000]\nprint(x_test.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"(1000,) (20,)\n","output_type":"stream"}]},{"cell_type":"code","source":"labels[1000]","metadata":{"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0.], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"# split the data into a training set and a validation set\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]","metadata":{"_uuid":"fc1c709458e9f4eb338a40c40c85dedba29c6fe8","trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Since we have our train-validation split ready, our next step is to create an embedding matrix from the precomputed Glove embeddings.\nFor convenience we are freezing the embedding layer i.e we will not be fine tuning the word embeddings. Feel free to test it out for better accuracy on very specific examples. From what can be seen, the Glove embeddings are universal features and tend to perform great in general.","metadata":{"_uuid":"2ba8039ec130a51f64bad77c718a7f2e91e13d19"}},{"cell_type":"code","source":"embeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"_uuid":"0620c11d2dab62329f250ecad40bcefbf57a7134","trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Found 400000 word vectors.\n","output_type":"stream"}]},{"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","metadata":{"_uuid":"07d064695cf65aaba497d6bb0dbd14dea220d533","trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Embedding\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)","metadata":{"_uuid":"74abe6ec0048d25c6169081f7cd409359588aee0","trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding = embedding_layer(inputs)\n\nprint(embedding.shape)\nreshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(embedding)\nprint(reshape.shape)\n\nconv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\nconv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\nconv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n\nmaxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\nmaxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\nmaxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n\nconcatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\nflatten = Flatten()(concatenated_tensor)\ndropout = Dropout(drop)(flatten)\noutput = Dense(units=20, activation='softmax')(dropout)\n\n# this creates a model that includes\nmodel = Model(inputs=inputs, outputs=output)\n\ncheckpoint = ModelCheckpoint('weights_cnn_sentece.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\nadam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\nmodel.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"_uuid":"e499119a397f180258ab0e2b8c5a6b47ef98fc7c","trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(?, 1000, 100)\n(?, 1000, 100, 1)\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            (None, 1000)         0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 1000, 100)    17407500    input_4[0][0]                    \n__________________________________________________________________________________________________\nreshape_4 (Reshape)             (None, 1000, 100, 1) 0           embedding_1[3][0]                \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 998, 1, 512)  154112      reshape_4[0][0]                  \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 997, 1, 512)  205312      reshape_4[0][0]                  \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 996, 1, 512)  256512      reshape_4[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_10 (MaxPooling2D) (None, 1, 1, 512)    0           conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_11 (MaxPooling2D) (None, 1, 1, 512)    0           conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_12 (MaxPooling2D) (None, 1, 1, 512)    0           conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 3, 1, 512)    0           max_pooling2d_10[0][0]           \n                                                                 max_pooling2d_11[0][0]           \n                                                                 max_pooling2d_12[0][0]           \n__________________________________________________________________________________________________\nflatten_4 (Flatten)             (None, 1536)         0           concatenate_4[0][0]              \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 1536)         0           flatten_4[0][0]                  \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 20)           30740       dropout_4[0][0]                  \n==================================================================================================\nTotal params: 18,054,176\nTrainable params: 646,676\nNon-trainable params: 17,407,500\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Traning Model...\")\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=20, verbose=1, callbacks=[checkpoint], validation_data=(x_val, y_val))\n","metadata":{"_uuid":"c3f99fb84e45c5fdf63607020c346902c340a31a","scrolled":true,"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Traning Model...\nTrain on 15998 samples, validate on 3999 samples\nEpoch 1/20\n15998/15998 [==============================] - 24s 2ms/step - loss: 3.4680 - acc: 0.1096 - val_loss: 2.0977 - val_acc: 0.4756\n\nEpoch 00001: val_acc improved from -inf to 0.47562, saving model to weights_cnn_sentece.hdf5\nEpoch 2/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 2.3293 - acc: 0.3014 - val_loss: 1.6210 - val_acc: 0.5849\n\nEpoch 00002: val_acc improved from 0.47562 to 0.58490, saving model to weights_cnn_sentece.hdf5\nEpoch 3/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 1.7530 - acc: 0.4575 - val_loss: 1.3266 - val_acc: 0.6737\n\nEpoch 00003: val_acc improved from 0.58490 to 0.67367, saving model to weights_cnn_sentece.hdf5\nEpoch 4/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 1.4215 - acc: 0.5598 - val_loss: 1.1504 - val_acc: 0.7019\n\nEpoch 00004: val_acc improved from 0.67367 to 0.70193, saving model to weights_cnn_sentece.hdf5\nEpoch 5/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 1.1927 - acc: 0.6395 - val_loss: 1.0376 - val_acc: 0.7217\n\nEpoch 00005: val_acc improved from 0.70193 to 0.72168, saving model to weights_cnn_sentece.hdf5\nEpoch 6/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 1.0445 - acc: 0.6866 - val_loss: 0.9538 - val_acc: 0.7444\n\nEpoch 00006: val_acc improved from 0.72168 to 0.74444, saving model to weights_cnn_sentece.hdf5\nEpoch 7/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.9377 - acc: 0.7188 - val_loss: 0.9009 - val_acc: 0.7432\n\nEpoch 00007: val_acc did not improve from 0.74444\nEpoch 8/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.8416 - acc: 0.7485 - val_loss: 0.8550 - val_acc: 0.7522\n\nEpoch 00008: val_acc improved from 0.74444 to 0.75219, saving model to weights_cnn_sentece.hdf5\nEpoch 9/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.7803 - acc: 0.7681 - val_loss: 0.8128 - val_acc: 0.7659\n\nEpoch 00009: val_acc improved from 0.75219 to 0.76594, saving model to weights_cnn_sentece.hdf5\nEpoch 10/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.7136 - acc: 0.7905 - val_loss: 0.7799 - val_acc: 0.7707\n\nEpoch 00010: val_acc improved from 0.76594 to 0.77069, saving model to weights_cnn_sentece.hdf5\nEpoch 11/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.6601 - acc: 0.8064 - val_loss: 0.7545 - val_acc: 0.7744\n\nEpoch 00011: val_acc improved from 0.77069 to 0.77444, saving model to weights_cnn_sentece.hdf5\nEpoch 12/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.6129 - acc: 0.8229 - val_loss: 0.7405 - val_acc: 0.7744\n\nEpoch 00012: val_acc did not improve from 0.77444\nEpoch 13/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.5720 - acc: 0.8372 - val_loss: 0.7160 - val_acc: 0.7817\n\nEpoch 00013: val_acc improved from 0.77444 to 0.78170, saving model to weights_cnn_sentece.hdf5\nEpoch 14/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.5308 - acc: 0.8476 - val_loss: 0.7009 - val_acc: 0.7809\n\nEpoch 00014: val_acc did not improve from 0.78170\nEpoch 15/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.5016 - acc: 0.8584 - val_loss: 0.6885 - val_acc: 0.7837\n\nEpoch 00015: val_acc improved from 0.78170 to 0.78370, saving model to weights_cnn_sentece.hdf5\nEpoch 16/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.4683 - acc: 0.8707 - val_loss: 0.6772 - val_acc: 0.7887\n\nEpoch 00016: val_acc improved from 0.78370 to 0.78870, saving model to weights_cnn_sentece.hdf5\nEpoch 17/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.4435 - acc: 0.8765 - val_loss: 0.6611 - val_acc: 0.7927\n\nEpoch 00017: val_acc improved from 0.78870 to 0.79270, saving model to weights_cnn_sentece.hdf5\nEpoch 18/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.4134 - acc: 0.8879 - val_loss: 0.6517 - val_acc: 0.7937\n\nEpoch 00018: val_acc improved from 0.79270 to 0.79370, saving model to weights_cnn_sentece.hdf5\nEpoch 19/20\n15998/15998 [==============================] - 23s 1ms/step - loss: 0.3903 - acc: 0.8942 - val_loss: 0.6544 - val_acc: 0.7952\n\nEpoch 00019: val_acc improved from 0.79370 to 0.79520, saving model to weights_cnn_sentece.hdf5\nEpoch 20/20\n15998/15998 [==============================] - 24s 1ms/step - loss: 0.3681 - acc: 0.8992 - val_loss: 0.6368 - val_acc: 0.7962\n\nEpoch 00020: val_acc improved from 0.79520 to 0.79620, saving model to weights_cnn_sentece.hdf5\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f20281019e8>"},"metadata":{}}]},{"cell_type":"code","source":"score, acc = model.evaluate(x_val, y_val)\nprint(\"Loss: \", score)\nprint(\"Accuracy: \", acc*100)","metadata":{"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"3999/3999 [==============================] - 2s 416us/step\nLoss:  0.6367613789468266\nAccuracy:  79.61990498071553\n","output_type":"stream"}]},{"cell_type":"code","source":"x_test=x_test.reshape(1, 1000)\npred=model.predict(x_test).argmax()","metadata":{"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"print(\"Actual label: \", y_test.argmax())\nprint(\"Predicted label: \", pred)","metadata":{"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"Actual label:  1\nPredicted label:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"print(labels[1000].argmax())","metadata":{"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"}]},{"cell_type":"code","source":"labels_index","metadata":{"trusted":true},"execution_count":103,"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"{'alt.atheism': 0,\n 'comp.graphics': 1,\n 'comp.os.ms-windows.misc': 2,\n 'comp.sys.ibm.pc.hardware': 3,\n 'comp.sys.mac.hardware': 4,\n 'comp.windows.x': 5,\n 'misc.forsale': 6,\n 'rec.autos': 7,\n 'rec.motorcycles': 8,\n 'rec.sport.baseball': 9,\n 'rec.sport.hockey': 10,\n 'sci.crypt': 11,\n 'sci.electronics': 12,\n 'sci.med': 13,\n 'sci.space': 14,\n 'soc.religion.christian': 15,\n 'talk.politics.guns': 16,\n 'talk.politics.mideast': 17,\n 'talk.politics.misc': 18,\n 'talk.religion.misc': 19}"},"metadata":{}}]},{"cell_type":"code","source":"print(texts[1000])","metadata":{"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"\n\n\n\t\t\tCALL FOR PRESENTATIONS\n\t\n      NAVY SCIENTIFIC VISUALIZATION AND VIRTUAL REALITY SEMINAR\n\n\t\t\tTuesday, June 22, 1993\n\n\t    Carderock Division, Naval Surface Warfare Center\n\t      (formerly the David Taylor Research Center)\n\t\t\t  Bethesda, Maryland\n\nSPONSOR: NESS (Navy Engineering Software System) is sponsoring a \none-day Navy Scientific Visualization and Virtual Reality Seminar.  \nThe purpose of the seminar is to present and exchange information for\nNavy-related scientific visualization and virtual reality programs, \nresearch, developments, and applications.\n\nPRESENTATIONS: Presentations are solicited on all aspects of \nNavy-related scientific visualization and virtual reality.  All \ncurrent work, works-in-progress, and proposed work by Navy \norganizations will be considered.  Four types of presentations are \navailable.\n\n     1. Regular presentation: 20-30 minutes in length\n     2. Short presentation: 10 minutes in length\n     3. Video presentation: a stand-alone videotape (author need not \n\tattend the seminar)\n     4. Scientific visualization or virtual reality demonstration (BYOH)\n\nAccepted presentations will not be published in any proceedings, \nhowever, viewgraphs and other materials will be reproduced for \nseminar attendees.\n\nABSTRACTS: Authors should submit a one page abstract and/or videotape to:\n\n     Robert Lipman\n     Naval Surface Warfare Center, Carderock Division\n     Code 2042\n     Bethesda, Maryland  20084-5000\n\n     VOICE (301) 227-3618;  FAX (301) 227-5753  \n     E-MAIL  lipman@oasys.dt.navy.mil\n\nAuthors should include the type of presentation, their affiliations, \naddresses, telephone and FAX numbers, and addresses.  Multi-author \npapers should designate one point of contact.\n\nDEADLINES: The abstact submission deadline is April 30, 1993.  \nNotification of acceptance will be sent by May 14, 1993.  \nMaterials for reproduction must be received by June 1, 1993.\n\nFor further information, contact Robert Lipman at the above address.\n\n\t  PLEASE DISTRIBUTE AS WIDELY AS POSSIBLE, THANKS.\n\n\n\n\nRobert Lipman                     | Internet: lipman@oasys.dt.navy.mil\nDavid Taylor Model Basin - CDNSWC |       or: lip@ocean.dt.navy.mil\nComputational Signatures and      | Voicenet: (301) 227-3618\n   Structures Group, Code 2042    | Factsnet: (301) 227-5753\nBethesda, Maryland  20084-5000    | Phishnet: stockings@long.legs\n\t\t\t\t   \nThe sixth sick shiek's sixth sheep's sick.\n\n[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"I hope this Kernel was helpful for you, any sort of feedback and comments are appreciated. Feel free to reach out in case something is unclear.\nthe entire code is also uploaded on my github : https://github.com/au1206/Convolutional-Neural-Networks-for-Sentence-Classification\n\n\nUntil next time, Happy learning :) . . .. ...","metadata":{"_uuid":"6dd11fdc4f1d2898adb4103d7794a7d4a3919d4c"}},{"cell_type":"code","source":"","metadata":{"_uuid":"4da2bc289e1d27d5225f68cb33352347e737c8a4"},"execution_count":null,"outputs":[]}]}